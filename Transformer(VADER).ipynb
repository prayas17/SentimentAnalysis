{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4985c7f3-0eb2-4652-998c-4d30b886e447",
   "metadata": {},
   "source": [
    "####  VADER (Valence Aware Dictionary and sEntiment Reasoner) pre-trained model for sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d85ad2-c620-43e5-abdf-d7c53e62ab8e",
   "metadata": {},
   "source": [
    "#### Use the SentimentIntensityAnalyzer class from the library to analyze the sentiment of your text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d05106-a0dd-448e-9a23-10c71de02172",
   "metadata": {},
   "source": [
    "####  VADER is a pre-trained model, which means it has already been trained on a large dataset of text samples to understand the sentiment of words and phrases. You can directly use it to analyze the sentiment of your text data without needing to train it on your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408c85e-2b88-47c8-a263-166f4c3aa408",
   "metadata": {},
   "source": [
    "#### Studies have shown VADER can outperform humans in classifying sentiment on social media text.\n",
    "#### It achieves an F1 score of 0.96,  meaning it's very good at correctly identifying positive, neutral, and negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd954ff7-9804-40d3-9421-b6e992a2088e",
   "metadata": {},
   "source": [
    "#### Reported accuracy can vary depending on the testing method. Some sources say it can reach over 90% accuracy on specific tasks, \n",
    "#### while others report accuracy in the 60% range for overall sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a19c14e1-acdd-4cc6-ad26-ab42e1841046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Prayas\n",
      "[nltk_data]     jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Prayas\n",
      "[nltk_data]     jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Prayas\n",
      "[nltk_data]     jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Function for preprocessing text data\n",
    "def preprocess_text(df, text_column):\n",
    "    # Lowercase the text\n",
    "    df[text_column] = df[text_column].str.lower()\n",
    "    \n",
    "    # Remove special characters, punctuation, and symbols\n",
    "    def remove_special_characters(text):\n",
    "        pattern = r'[^a-zA-Z0-9\\s]'  # Define regex pattern\n",
    "        return re.sub(pattern, '', text)\n",
    "    \n",
    "    df[text_column] = df[text_column].apply(remove_special_characters)\n",
    "    \n",
    "    # Remove numbers\n",
    "    def remove_numbers(text):\n",
    "        pattern = r'\\b\\d+\\b'  # Define regex pattern to match any standalone number\n",
    "        return re.sub(pattern, '', text)\n",
    "    \n",
    "    df[text_column] = df[text_column].apply(remove_numbers)\n",
    "    \n",
    "    # Download stopwords list\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    def remove_stopwords(text):\n",
    "        cleaned_text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "        return cleaned_text\n",
    "    \n",
    "    df[text_column] = df[text_column].apply(remove_stopwords)\n",
    "    \n",
    "    # Tokenize text\n",
    "    nltk.download('punkt')\n",
    "    def tokenize_text(text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    df['token'] = df[text_column].apply(tokenize_text)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    def stem_tokens(tokens):\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "    \n",
    "    df['stemmed_token'] = df['token'].apply(stem_tokens)\n",
    "    \n",
    "    # Lemmatization\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def lemmatize_tokens(tokens):\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return lemmatized_tokens\n",
    "    \n",
    "    df['lemmatized_token'] = df['stemmed_token'].apply(lemmatize_tokens)\n",
    "    \n",
    "    # Combine tokens into cleaned text\n",
    "    df['cleaned_text'] = df['lemmatized_token'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"hate.csv\" , encoding='latin1')  \n",
    "\n",
    "# Preprocess text data\n",
    "data = preprocess_text(data, 'comment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ac5aa9-8343-4bbf-9542-f6c5054c27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment scores\n",
    "def get_sentiment_scores(text):\n",
    "    # Analyze sentiment\n",
    "    sentiment_scores = analyzer.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "# Apply VADER to your dataset\n",
    "data['sentiment_scores'] = data['cleaned_text'].apply(get_sentiment_scores)\n",
    "\n",
    "# Extract compound score\n",
    "data['compound_score'] = data['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Classify sentiment based on compound score\n",
    "data['sentiment'] = data['compound_score'].apply(lambda score: 'positive' if score >= 0 else 'negative')\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2262cffb-bf61-4111-b551-78498b774e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Prayas\n",
      "[nltk_data]     jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence:  I love you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization (split the text into words/tokens)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Take input from the user\n",
    "user_input = input(\"Enter a sentence: \")\n",
    "\n",
    "# Preprocess the input text\n",
    "preprocessed_input = preprocess_text(user_input)\n",
    "\n",
    "# Analyze sentiment using VADER\n",
    "sentiment_scores = analyzer.polarity_scores(preprocessed_input)\n",
    "\n",
    "# Determine overall sentiment\n",
    "if sentiment_scores['compound'] >= 0.05:\n",
    "    print(\"Overall sentiment: Positive\")\n",
    "elif sentiment_scores['compound'] <= -0.05:\n",
    "    print(\"Overall sentiment: Negative\")\n",
    "else:\n",
    "    print(\"Overall sentiment: Neutral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4925f2-c826-440a-917e-1b14e1ed8c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
